{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_commonlit_roberta_andrey_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMkGbD6WKslW94h/yD0qsqZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ny-yo/kaggle-CommonLit-Readability-Prize/blob/main/01_commonlit_roberta_andrey_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYvcCzvX69XS"
      },
      "source": [
        "ベースkernel  \n",
        "https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NYk6Mvi4RHU"
      },
      "source": [
        "変更履歴  \n",
        "07/03 ver01 新規作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WskC91WmDPDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "319f790b-06ac-4b97-f600-df08c72d4261"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5gaTvncAwk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf12887-106d-47ca-b95d-7f1d3c3244f1"
      },
      "source": [
        "from google.colab import drive #インポート\n",
        "drive.mount('/content/drive/') #GoogleDriveのマウント"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FPTrS2eD9vC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab06832e-ba51-4f4a-c581-f8f958bd815f"
      },
      "source": [
        "# download API key from google drive\n",
        "## Original: https://colab.research.google.com/drive/1eufc8aNCdjHbrBhuy7M7X6BGyzAyRbrF#scrollTo=y5_288BYp6H1\n",
        "## When you run for the first time, you will see a link to authenticate.\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2XtAVMeEHMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8cfba4-39f3-4522-891a-b793c10046de"
      },
      "source": [
        "!kaggle competitions list"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "contradictory-my-dear-watson                   2030-07-01 23:59:00  Getting Started     Prizes        201           False  \n",
            "gan-getting-started                            2030-07-01 23:59:00  Getting Started     Prizes        339           False  \n",
            "tpu-getting-started                            2030-06-03 23:59:00  Getting Started  Knowledge        987           False  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       6167           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge      51833            True  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge      13543            True  \n",
            "connectx                                       2030-01-01 00:00:00  Getting Started  Knowledge       1003           False  \n",
            "nlp-getting-started                            2030-01-01 00:00:00  Getting Started  Knowledge       3371            True  \n",
            "competitive-data-science-predict-future-sales  2022-12-31 23:59:00  Playground           Kudos      12035            True  \n",
            "g2net-gravitational-wave-detection             2021-09-29 23:59:00  Research           $15,000         98           False  \n",
            "optiver-realized-volatility-prediction         2021-09-27 23:59:00  Featured          $100,000        360           False  \n",
            "jane-street-market-prediction                  2021-08-23 23:59:00  Featured          $100,000       4245            True  \n",
            "siim-covid19-detection                         2021-08-09 23:59:00  Featured          $100,000        824           False  \n",
            "google-smartphone-decimeter-challenge          2021-08-04 23:59:00  Research           $10,000        604            True  \n",
            "commonlitreadabilityprize                      2021-08-02 23:59:00  Featured           $60,000       2786            True  \n",
            "mlb-player-digital-engagement-forecasting      2021-07-31 23:59:00  Featured           $50,000        521           False  \n",
            "tabular-playground-series-jul-2021             2021-07-31 23:59:00  Playground            Swag        268           False  \n",
            "seti-breakthrough-listen                       2021-07-28 23:59:00  Research           $15,000       1109           False  \n",
            "hungry-geese                                   2021-07-26 23:59:00  Playground          Prizes        800           False  \n",
            "tabular-playground-series-jun-2021             2021-06-30 23:59:00  Playground            Swag       1171           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYjRmSmDEcm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f964b91f-1cad-46ab-b983-7fc7d553759a"
      },
      "source": [
        "!kaggle competitions download -c commonlitreadabilityprize"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "sample_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KFEnUGXFpr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbf83a7-c9f1-4355-84e9-142be25da71a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYAEN8e9ohqp"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gc\n",
        "gc.enable()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx1bkh1SmFQL"
      },
      "source": [
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 248\n",
        "EVAL_SCHEDULE = [(0.5, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1, 1)]\n",
        "ROBERTA_PATH = \"roberta-base\"\n",
        "TOKENIZER_PATH = \"roberta-base\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HkRY0_zng0e"
      },
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm2b-dewpdfo",
        "outputId": "85ece97f-bde1-430c-d057-36be90f5652e"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/train.csv.zip\")\n",
        "print(train_df.shape)\n",
        "# Remove incomplete entries if any.\n",
        "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
        "              inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "print(train_df.shape)\n",
        "\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "submission_df = pd.read_csv(\"/content/sample_submission.csv\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2834, 6)\n",
            "(2833, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyRyMURgqsNo"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAl5AZRGrBCG"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S9FGB21qy23"
      },
      "source": [
        "class LitDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df\n",
        "        self.inference_only = inference_only\n",
        "        self.text = df.excerpt.tolist()\n",
        "\n",
        "        if not self.inference_only:\n",
        "            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n",
        "        \n",
        "        #batch_encode_plusの説明\n",
        "        #https://qiita.com/ichiroex/items/6e305a5d5bed7d715c2f\n",
        "        self.encoded = tokenizer.batch_encode_plus(\n",
        "            self.text,\n",
        "            padding = \"max_length\",\n",
        "            max_length = MAX_LEN,\n",
        "            truncation = True,\n",
        "            return_attention_mask = True\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = torch.tensor(self.encoded[\"input_ids\"][index])\n",
        "        attention_mask = torch.tensor(self.encoded[\"attention_mask\"][index])\n",
        "\n",
        "        if self.inference_only:\n",
        "            return (input_ids, attention_mask)\n",
        "        else:\n",
        "            target = self.target[index]\n",
        "            return (input_ids, attention_mask, target)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8DAOQSN0w0e"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EErTA0n7wFM1"
      },
      "source": [
        "class LitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
        "        config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})\n",
        "        \n",
        "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(768, 1)\n",
        "        )\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        #robertaの最終層だけを取り出す\n",
        "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
        "\n",
        "        weights = self.attention(last_layer_hidden_states)\n",
        "\n",
        "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)  \n",
        "\n",
        "        return self.regressor(context_vector)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyCiU9yJ5Y2T"
      },
      "source": [
        "def eval_mse(model, data_loader):\n",
        "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()            \n",
        "    mse_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)                        \n",
        "            target = target.to(DEVICE)           \n",
        "            \n",
        "            pred = model(input_ids, attention_mask)                       \n",
        "\n",
        "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
        "                \n",
        "\n",
        "    return mse_sum / len(data_loader.dataset)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9XgjX4I5hWG"
      },
      "source": [
        "def predict(model, data_loader):\n",
        "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    result = np.zeros(len(data_loader.dataset))    \n",
        "    index = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "                        \n",
        "            pred = model(input_ids, attention_mask)                        \n",
        "\n",
        "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
        "            index += pred.shape[0]\n",
        "\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSf2GRdT5tn0"
      },
      "source": [
        "def train(model, model_path, train_loader, val_loader,\n",
        "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
        "    best_val_rmse = None\n",
        "    best_epoch = 0\n",
        "    step = 0\n",
        "    last_eval_step = 0\n",
        "    eval_period = EVAL_SCHEDULE[0][1]    \n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):                           \n",
        "        val_rmse = None         \n",
        "\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)            \n",
        "            target = target.to(DEVICE)                        \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "            pred = model(input_ids, attention_mask)\n",
        "                                                        \n",
        "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
        "                        \n",
        "            mse.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if step >= last_eval_step + eval_period:\n",
        "                # Evaluate the model on val_loader.\n",
        "                elapsed_seconds = time.time() - start\n",
        "                num_steps = step - last_eval_step\n",
        "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
        "                last_eval_step = step\n",
        "                \n",
        "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
        "\n",
        "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
        "                      f\"val_rmse: {val_rmse:0.4}\")\n",
        "\n",
        "                for rmse, period in EVAL_SCHEDULE:\n",
        "                    if val_rmse >= rmse:\n",
        "                        eval_period = period\n",
        "                        break                               \n",
        "                \n",
        "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
        "                else:       \n",
        "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
        "                          f\"(from epoch {best_epoch})\")                                    \n",
        "                    \n",
        "                start = time.time()\n",
        "                                            \n",
        "            step += 1\n",
        "                        \n",
        "    \n",
        "    return best_val_rmse"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cf3ApfJ6Kzd"
      },
      "source": [
        "def create_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "    \n",
        "    roberta_parameters = named_parameters[:197]    \n",
        "    attention_parameters = named_parameters[199:203]\n",
        "    regressor_parameters = named_parameters[203:]\n",
        "        \n",
        "    attention_group = [params for (name, params) in attention_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": attention_group})\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
        "\n",
        "        lr = 2e-5\n",
        "\n",
        "        if layer_num >= 69:        \n",
        "            lr = 5e-5\n",
        "\n",
        "        if layer_num >= 133:\n",
        "            lr = 1e-4\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlqeXhiP6ODE",
        "outputId": "0bb5140d-e1e1-4220-8bed-6a990c21ad9e"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "SEED = 1000\n",
        "list_val_rmse = []\n",
        "\n",
        "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
        "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
        "    model_path = f\"/content/drive/MyDrive/01-andrey-base-model_{fold + 1}.pth\"\n",
        "    \n",
        "    set_random_seed(SEED + fold)\n",
        "    \n",
        "    train_dataset = LitDataset(train_df.loc[train_indices])    \n",
        "    val_dataset = LitDataset(train_df.loc[val_indices])    \n",
        "        \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              drop_last=True, shuffle=True, num_workers=2)    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            drop_last=False, shuffle=False, num_workers=2)    \n",
        "        \n",
        "    set_random_seed(SEED + fold)    \n",
        "    \n",
        "    model = LitModel().to(DEVICE)\n",
        "    \n",
        "    optimizer = create_optimizer(model)                        \n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "        num_warmup_steps=50)    \n",
        "    \n",
        "    list_val_rmse.append(train(model, model_path, train_loader,\n",
        "                               val_loader, optimizer, scheduler=scheduler))\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"\\nPerformance estimates:\")\n",
        "    print(list_val_rmse)\n",
        "    print(\"Mean:\", np.array(list_val_rmse).mean())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.89 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9384\n",
            "New best_val_rmse: 0.9384\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.8438\n",
            "New best_val_rmse: 0.8438\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6332\n",
            "New best_val_rmse: 0.6332\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6437\n",
            "Still best_val_rmse: 0.6332 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.5707\n",
            "New best_val_rmse: 0.5707\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5282\n",
            "New best_val_rmse: 0.5282\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5513\n",
            "Still best_val_rmse: 0.5282 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5392\n",
            "Still best_val_rmse: 0.5282 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5223\n",
            "New best_val_rmse: 0.5223\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5129\n",
            "New best_val_rmse: 0.5129\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5308\n",
            "Still best_val_rmse: 0.5129 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5237\n",
            "Still best_val_rmse: 0.5129 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5204\n",
            "Still best_val_rmse: 0.5129 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.5006\n",
            "New best_val_rmse: 0.5006\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.4938\n",
            "New best_val_rmse: 0.4938\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 107 val_rmse: 0.4919\n",
            "New best_val_rmse: 0.4919\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.505\n",
            "Still best_val_rmse: 0.4919 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.4823\n",
            "New best_val_rmse: 0.4823\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 135 val_rmse: 0.4879\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 139 val_rmse: 0.4851\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.7 seconds\n",
            "Epoch: 2 batch_num: 2 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.4842\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.5012\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.4912\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 34 val_rmse: 0.4835\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.4847\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.4888\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.4944\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4955\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4864\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4953\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4879\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4856\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4862\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.4906\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.492\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4889\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.488\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4865\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4857\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4856\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4856\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4857\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4823 (from epoch 1)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4823114265044416]\n",
            "Mean: 0.4823114265044416\n",
            "\n",
            "Fold 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.84 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9605\n",
            "New best_val_rmse: 0.9605\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7348\n",
            "New best_val_rmse: 0.7348\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7169\n",
            "New best_val_rmse: 0.7169\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6632\n",
            "New best_val_rmse: 0.6632\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6239\n",
            "New best_val_rmse: 0.6239\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.6574\n",
            "Still best_val_rmse: 0.6239 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.582\n",
            "New best_val_rmse: 0.582\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5612\n",
            "New best_val_rmse: 0.5612\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.537\n",
            "New best_val_rmse: 0.537\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5049\n",
            "New best_val_rmse: 0.5049\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5096\n",
            "Still best_val_rmse: 0.5049 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5102\n",
            "Still best_val_rmse: 0.5049 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5208\n",
            "Still best_val_rmse: 0.5049 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.4934\n",
            "New best_val_rmse: 0.4934\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 91 val_rmse: 0.4903\n",
            "New best_val_rmse: 0.4903\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.4858\n",
            "New best_val_rmse: 0.4858\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 103 val_rmse: 0.4817\n",
            "New best_val_rmse: 0.4817\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 107 val_rmse: 0.4773\n",
            "New best_val_rmse: 0.4773\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 1 batch_num: 109 val_rmse: 0.4857\n",
            "Still best_val_rmse: 0.4773 (from epoch 1)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 113 val_rmse: 0.5072\n",
            "Still best_val_rmse: 0.4773 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 129 val_rmse: 0.4709\n",
            "New best_val_rmse: 0.4709\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.4789\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "2 steps took 0.785 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 1 batch_num: 135 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 1 batch_num: 137 val_rmse: 0.4969\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "8 steps took 3.28 seconds\n",
            "Epoch: 2 batch_num: 4 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 8 val_rmse: 0.4715\n",
            "Still best_val_rmse: 0.4709 (from epoch 1)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4684\n",
            "New best_val_rmse: 0.4684\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 11 val_rmse: 0.4697\n",
            "Still best_val_rmse: 0.4684 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 12 val_rmse: 0.4745\n",
            "Still best_val_rmse: 0.4684 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 14 val_rmse: 0.4874\n",
            "Still best_val_rmse: 0.4684 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.4699\n",
            "Still best_val_rmse: 0.4684 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 19 val_rmse: 0.4665\n",
            "New best_val_rmse: 0.4665\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 20 val_rmse: 0.468\n",
            "Still best_val_rmse: 0.4665 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 21 val_rmse: 0.4701\n",
            "Still best_val_rmse: 0.4665 (from epoch 2)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 2 batch_num: 23 val_rmse: 0.4726\n",
            "Still best_val_rmse: 0.4665 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 25 val_rmse: 0.4687\n",
            "Still best_val_rmse: 0.4665 (from epoch 2)\n",
            "\n",
            "1 steps took 0.39 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.4657\n",
            "New best_val_rmse: 0.4657\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 27 val_rmse: 0.465\n",
            "New best_val_rmse: 0.465\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 28 val_rmse: 0.467\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 29 val_rmse: 0.4729\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 31 val_rmse: 0.4876\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 35 val_rmse: 0.4924\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 43 val_rmse: 0.4749\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 45 val_rmse: 0.4755\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 47 val_rmse: 0.469\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 48 val_rmse: 0.4691\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 49 val_rmse: 0.474\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 51 val_rmse: 0.4922\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 59 val_rmse: 0.4678\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 60 val_rmse: 0.4663\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 61 val_rmse: 0.4659\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4658\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 63 val_rmse: 0.4655\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 64 val_rmse: 0.4652\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 65 val_rmse: 0.4652\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4655\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 67 val_rmse: 0.4662\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4674\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 69 val_rmse: 0.4694\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.395 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4708\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 72 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 76 val_rmse: 0.4729\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4694\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 79 val_rmse: 0.468\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 80 val_rmse: 0.4671\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 81 val_rmse: 0.4669\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 83 val_rmse: 0.4666\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 84 val_rmse: 0.4665\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.39 seconds\n",
            "Epoch: 2 batch_num: 85 val_rmse: 0.4666\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4666\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 87 val_rmse: 0.4667\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 88 val_rmse: 0.4668\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 89 val_rmse: 0.4669\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.4671\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 91 val_rmse: 0.4673\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 92 val_rmse: 0.4676\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.394 seconds\n",
            "Epoch: 2 batch_num: 93 val_rmse: 0.468\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.391 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.4688\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.393 seconds\n",
            "Epoch: 2 batch_num: 95 val_rmse: 0.4698\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "1 steps took 0.392 seconds\n",
            "Epoch: 2 batch_num: 96 val_rmse: 0.4709\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.785 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4734\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 100 val_rmse: 0.475\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 104 val_rmse: 0.4756\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 108 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 112 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4747\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4743\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4735\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4733\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4731\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4729\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.465 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4823114265044416, 0.46504391109194415]\n",
            "Mean: 0.47367766879819284\n",
            "\n",
            "Fold 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.85 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.021\n",
            "New best_val_rmse: 1.021\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 1.065\n",
            "Still best_val_rmse: 1.021 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6681\n",
            "New best_val_rmse: 0.6681\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6229\n",
            "New best_val_rmse: 0.6229\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6371\n",
            "Still best_val_rmse: 0.6229 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5993\n",
            "New best_val_rmse: 0.5993\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5311\n",
            "New best_val_rmse: 0.5311\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5808\n",
            "Still best_val_rmse: 0.5311 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.5132\n",
            "New best_val_rmse: 0.5132\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5562\n",
            "Still best_val_rmse: 0.5132 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5225\n",
            "Still best_val_rmse: 0.5132 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5227\n",
            "Still best_val_rmse: 0.5132 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5543\n",
            "Still best_val_rmse: 0.5132 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.4972\n",
            "New best_val_rmse: 0.4972\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 91 val_rmse: 0.5007\n",
            "Still best_val_rmse: 0.4972 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 107 val_rmse: 0.4969\n",
            "New best_val_rmse: 0.4969\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.4842\n",
            "New best_val_rmse: 0.4842\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 119 val_rmse: 0.4982\n",
            "Still best_val_rmse: 0.4842 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 127 val_rmse: 0.4826\n",
            "New best_val_rmse: 0.4826\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.514\n",
            "Still best_val_rmse: 0.4826 (from epoch 1)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.4761\n",
            "New best_val_rmse: 0.4761\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 8 val_rmse: 0.4874\n",
            "Still best_val_rmse: 0.4761 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 12 val_rmse: 0.4824\n",
            "Still best_val_rmse: 0.4761 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 16 val_rmse: 0.4838\n",
            "Still best_val_rmse: 0.4761 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 20 val_rmse: 0.4747\n",
            "New best_val_rmse: 0.4747\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 2 batch_num: 22 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 24 val_rmse: 0.4921\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 32 val_rmse: 0.4919\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 40 val_rmse: 0.4943\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 48 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.4787\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 52 val_rmse: 0.4779\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 56 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.793 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 60 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.795 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4781\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 64 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4761\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4753\n",
            "Still best_val_rmse: 0.4747 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4746\n",
            "New best_val_rmse: 0.4746\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 2 batch_num: 72 val_rmse: 0.4738\n",
            "New best_val_rmse: 0.4738\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.4733\n",
            "New best_val_rmse: 0.4733\n",
            "\n",
            "2 steps took 0.792 seconds\n",
            "Epoch: 2 batch_num: 76 val_rmse: 0.4726\n",
            "New best_val_rmse: 0.4726\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4726\n",
            "New best_val_rmse: 0.4726\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 80 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.785 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4734\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 2 batch_num: 84 val_rmse: 0.4742\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4742\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 88 val_rmse: 0.4747\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 92 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.4732\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 96 val_rmse: 0.4726\n",
            "Still best_val_rmse: 0.4726 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4719\n",
            "New best_val_rmse: 0.4719\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 100 val_rmse: 0.4713\n",
            "New best_val_rmse: 0.4713\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.471\n",
            "New best_val_rmse: 0.471\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 104 val_rmse: 0.4708\n",
            "New best_val_rmse: 0.4708\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4708\n",
            "New best_val_rmse: 0.4708\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 108 val_rmse: 0.4709\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4711\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 112 val_rmse: 0.4712\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4713\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4715\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4717\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4718\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4717\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4716\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4715\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4714\n",
            "Still best_val_rmse: 0.4708 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4823114265044416, 0.46504391109194415, 0.47078830284675466]\n",
            "Mean: 0.47271454681438013\n",
            "\n",
            "Fold 4/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.86 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9691\n",
            "New best_val_rmse: 0.9691\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.8864\n",
            "New best_val_rmse: 0.8864\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.8383\n",
            "New best_val_rmse: 0.8383\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.7234\n",
            "New best_val_rmse: 0.7234\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6247\n",
            "New best_val_rmse: 0.6247\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.625\n",
            "Still best_val_rmse: 0.6247 (from epoch 0)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5602\n",
            "New best_val_rmse: 0.5602\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.6244\n",
            "Still best_val_rmse: 0.5602 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.548\n",
            "New best_val_rmse: 0.548\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5286\n",
            "New best_val_rmse: 0.5286\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5349\n",
            "Still best_val_rmse: 0.5286 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5204\n",
            "New best_val_rmse: 0.5204\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.51\n",
            "New best_val_rmse: 0.51\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.5374\n",
            "Still best_val_rmse: 0.51 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.5072\n",
            "New best_val_rmse: 0.5072\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.4942\n",
            "New best_val_rmse: 0.4942\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 123 val_rmse: 0.4951\n",
            "Still best_val_rmse: 0.4942 (from epoch 1)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 1 batch_num: 131 val_rmse: 0.4963\n",
            "Still best_val_rmse: 0.4942 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 139 val_rmse: 0.4938\n",
            "New best_val_rmse: 0.4938\n",
            "\n",
            "8 steps took 3.29 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.496\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 14 val_rmse: 0.5059\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 2 batch_num: 30 val_rmse: 0.5139\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.5105\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 62 val_rmse: 0.4964\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.5033\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4927\n",
            "New best_val_rmse: 0.4927\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.4963\n",
            "Still best_val_rmse: 0.4927 (from epoch 2)\n",
            "\n",
            "8 steps took 3.16 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4929\n",
            "Still best_val_rmse: 0.4927 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4892\n",
            "New best_val_rmse: 0.4892\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4884\n",
            "New best_val_rmse: 0.4884\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.488\n",
            "New best_val_rmse: 0.488\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4879\n",
            "New best_val_rmse: 0.4879\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4879\n",
            "Still best_val_rmse: 0.4879 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4881\n",
            "Still best_val_rmse: 0.4879 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4881\n",
            "Still best_val_rmse: 0.4879 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.488\n",
            "Still best_val_rmse: 0.4879 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4823114265044416, 0.46504391109194415, 0.47078830284675466, 0.4878951380783082]\n",
            "Mean: 0.4765096946303622\n",
            "\n",
            "Fold 5/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.87 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9208\n",
            "New best_val_rmse: 0.9208\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7644\n",
            "New best_val_rmse: 0.7644\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.6432\n",
            "New best_val_rmse: 0.6432\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6143\n",
            "New best_val_rmse: 0.6143\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.5717\n",
            "New best_val_rmse: 0.5717\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.5819\n",
            "Still best_val_rmse: 0.5717 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5923\n",
            "Still best_val_rmse: 0.5717 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5499\n",
            "New best_val_rmse: 0.5499\n",
            "\n",
            "16 steps took 6.48 seconds\n",
            "Epoch: 1 batch_num: 3 val_rmse: 0.6323\n",
            "Still best_val_rmse: 0.5499 (from epoch 0)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 19 val_rmse: 0.5454\n",
            "New best_val_rmse: 0.5454\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 35 val_rmse: 0.5202\n",
            "New best_val_rmse: 0.5202\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 51 val_rmse: 0.5297\n",
            "Still best_val_rmse: 0.5202 (from epoch 1)\n",
            "\n",
            "16 steps took 6.34 seconds\n",
            "Epoch: 1 batch_num: 67 val_rmse: 0.5142\n",
            "New best_val_rmse: 0.5142\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 83 val_rmse: 0.526\n",
            "Still best_val_rmse: 0.5142 (from epoch 1)\n",
            "\n",
            "16 steps took 6.33 seconds\n",
            "Epoch: 1 batch_num: 99 val_rmse: 0.4979\n",
            "New best_val_rmse: 0.4979\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 107 val_rmse: 0.4958\n",
            "New best_val_rmse: 0.4958\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 115 val_rmse: 0.488\n",
            "New best_val_rmse: 0.488\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 1 batch_num: 119 val_rmse: 0.4927\n",
            "Still best_val_rmse: 0.488 (from epoch 1)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 1 batch_num: 127 val_rmse: 0.5004\n",
            "Still best_val_rmse: 0.488 (from epoch 1)\n",
            "\n",
            "16 steps took 6.48 seconds\n",
            "Epoch: 2 batch_num: 2 val_rmse: 0.4853\n",
            "New best_val_rmse: 0.4853\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 6 val_rmse: 0.4822\n",
            "New best_val_rmse: 0.4822\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4983\n",
            "Still best_val_rmse: 0.4822 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.4849\n",
            "Still best_val_rmse: 0.4822 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 22 val_rmse: 0.4767\n",
            "New best_val_rmse: 0.4767\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 24 val_rmse: 0.4756\n",
            "New best_val_rmse: 0.4756\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 28 val_rmse: 0.4905\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "8 steps took 3.17 seconds\n",
            "Epoch: 2 batch_num: 36 val_rmse: 0.4828\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 40 val_rmse: 0.4768\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.785 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 44 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 46 val_rmse: 0.49\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.4879\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 54 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 56 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4775\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 60 val_rmse: 0.4804\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 64 val_rmse: 0.4859\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.4804\n",
            "Still best_val_rmse: 0.4756 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 72 val_rmse: 0.4741\n",
            "New best_val_rmse: 0.4741\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.474\n",
            "New best_val_rmse: 0.474\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 76 val_rmse: 0.4741\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4742\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 80 val_rmse: 0.4751\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.4772\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 84 val_rmse: 0.4793\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 86 val_rmse: 0.4811\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.4841\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "4 steps took 1.59 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.485\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4866\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4876\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4838\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "4 steps took 1.58 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4795\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 112 val_rmse: 0.4779\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4771\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.79 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.787 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.785 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4758\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.791 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.786 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.789 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "2 steps took 0.788 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4757\n",
            "Still best_val_rmse: 0.474 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.4823114265044416, 0.46504391109194415, 0.47078830284675466, 0.4878951380783082, 0.47400883621435297]\n",
            "Mean: 0.4760095229471603\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}