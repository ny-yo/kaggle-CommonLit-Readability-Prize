{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sturdy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rFdSlkfZaFdBWkLVY5bsLfrcO0dsbVg4",
      "authorship_tag": "ABX9TyOjhGZ6yfuFOqTsoniWtvjp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ny-yo/kaggle-CommonLit-Readability-Prize/blob/main/bert_sturdy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qu8-g9beVfv",
        "outputId": "1811c891-27cc-4f48-d1ee-a7a953498e64"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0.dev0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UihZThljxtgS",
        "outputId": "7704f67b-f8d7-452c-9ac2-ae1113f8ffe3"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.7.0.dev0-cp37-none-any.whl size=2308934 sha256=7254d1beaa771cde181664f43a3ba601b006dd2c4bcd2d8615c254e50febd1a7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yxdua_8z/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.7.0.dev0\n",
            "    Uninstalling transformers-4.7.0.dev0:\n",
            "      Successfully uninstalled transformers-4.7.0.dev0\n",
            "Successfully installed transformers-4.7.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HdiCBrjfO1B",
        "outputId": "fb381e8d-c8d6-4865-f8de-3a4879f5a043"
      },
      "source": [
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "!pip install fugashi ipadic"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "aptitude is already the newest version (0.8.10-6ubuntu1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "No packages will be installed, upgraded, or removed.\n",
            "0 packages upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 0 B of archives. After unpacking 0 B will be used.\n",
            "                            \n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxwTgWw0kNG7"
      },
      "source": [
        "import torch\n",
        "# GPUが使えれば利用する設定\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Bh3Uf3kbox",
        "outputId": "15d61879-1bfc-4dd3-e58e-86fec225d0ec"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import re\n",
        "import csv\n",
        "import tarfile\n",
        "import transformers\n",
        "import numpy as np\n",
        "# データのダウンロード（カレントディレクトリに圧縮ファイルがダウンロードされる）\n",
        "urllib.request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\", \"ldcc-20140209.tar.gz\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ldcc-20140209.tar.gz', <http.client.HTTPMessage at 0x7f28c1dfa810>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0sLTNuCkufL"
      },
      "source": [
        "# ダウンロードした圧縮ファイルのパスを設定\n",
        "tgz_fname = \"ldcc-20140209.tar.gz\" \n",
        "# 2つをニュースメディアのジャンルを選定\n",
        "target_genre = [\"it-life-hack\", \"sports-watch\"] \n",
        "#処理をした結果を保存するファイル名 \n",
        "tsv_fname = \"all_text.tsv\" \n",
        "\n",
        "\n",
        "# 処理部分-------\n",
        "brackets_tail = re.compile('【[^】]*】$')\n",
        "brackets_head = re.compile('^【[^】]*】')\n",
        "\n",
        "def remove_brackets(inp):\n",
        "    output = re.sub(brackets_head, '', re.sub(brackets_tail, '', inp))\n",
        "    return output\n",
        "\n",
        "def read_title(f):\n",
        "    # 2行スキップ\n",
        "    next(f)\n",
        "    next(f)\n",
        "    title = next(f) # 3行目を返す\n",
        "    title = remove_brackets(title.decode('utf-8'))\n",
        "    return title[:-1]\n",
        "\n",
        "zero_fnames = []\n",
        "one_fnames = []\n",
        "\n",
        "with tarfile.open(tgz_fname) as tf:\n",
        "    # 対象ファイルの選定\n",
        "    for ti in tf:\n",
        "        # ライセンスファイルはスキップ\n",
        "        if \"LICENSE.txt\" in ti.name:\n",
        "            continue\n",
        "        if target_genre[0] in ti.name and ti.name.endswith(\".txt\"):\n",
        "            zero_fnames.append(ti.name)\n",
        "            continue\n",
        "        if target_genre[1] in ti.name and ti.name.endswith(\".txt\"):\n",
        "            one_fnames.append(ti.name)\n",
        "    with open(tsv_fname, \"w\") as wf:\n",
        "        writer = csv.writer(wf, delimiter='\\t')\n",
        "        # ラベル 0\n",
        "        for name in zero_fnames:\n",
        "            f = tf.extractfile(name)\n",
        "            title = read_title(f)\n",
        "            row = [target_genre[0], 0, '', title]\n",
        "            writer.writerow(row)\n",
        "        # ラベル 1\n",
        "        for name in one_fnames:\n",
        "            f = tf.extractfile(name)\n",
        "            title = read_title(f)\n",
        "            row = [target_genre[1], 1, '', title]\n",
        "            writer.writerow(row)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "FOf5THBUk4H4",
        "outputId": "6aadfbf3-4374-40cf-f1fe-761999e26d0e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#データの読み込み\n",
        "df = pd.read_csv(\"all_text.tsv\", delimiter='\\t', header=None, names=['media_name', 'label', 'NaN', 'sentence'])\n",
        "\n",
        "#データの確認\n",
        "print(f'データサイズ： {df.shape}')\n",
        "df.sample(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "データサイズ： (1770, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>media_name</th>\n",
              "      <th>label</th>\n",
              "      <th>NaN</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>sports-watch</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>内田、背水のセカンドレグへ「こんなところで投げたくない」</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>879</th>\n",
              "      <td>sports-watch</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>決戦から一夜、真央「3回飛ぶって決めてました」</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>it-life-hack</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>おはこんです！紺子のＭＭＤ動画が流れる予定ですっ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1586</th>\n",
              "      <td>sports-watch</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>横浜・中畑監督を痛烈に批判、「精神面を問題にするようじゃ最下位」</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1656</th>\n",
              "      <td>sports-watch</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>“テコンドー界のエリカ様”平成生まれの美少女アスリート＝笠原江梨香に注目集まる</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>sports-watch</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ダル実戦デビューに「あれが本物のダルビッシュだと思ったら大きな間違い」</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>it-life-hack</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>もちろんカレー臭はしなかった！ 品質もそこそこなインド製携帯電話</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>it-life-hack</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Twitterの影響力マジパねぇ！　東大駒場校舎で行われた無料耳かきサービスに人だかり</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>it-life-hack</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>孫社長が新スマホを大々的に宣伝！放射線測定スマホを開発した理由</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>it-life-hack</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>タダ者じゃない！7インチタブレット「eden TAB」の魅力をさぐる</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        media_name  label  NaN                                     sentence\n",
              "1211  sports-watch      1  NaN                 内田、背水のセカンドレグへ「こんなところで投げたくない」\n",
              "879   sports-watch      1  NaN                      決戦から一夜、真央「3回飛ぶって決めてました」\n",
              "205   it-life-hack      0  NaN                     おはこんです！紺子のＭＭＤ動画が流れる予定ですっ\n",
              "1586  sports-watch      1  NaN             横浜・中畑監督を痛烈に批判、「精神面を問題にするようじゃ最下位」\n",
              "1656  sports-watch      1  NaN      “テコンドー界のエリカ様”平成生まれの美少女アスリート＝笠原江梨香に注目集まる\n",
              "1594  sports-watch      1  NaN          ダル実戦デビューに「あれが本物のダルビッシュだと思ったら大きな間違い」\n",
              "299   it-life-hack      0  NaN             もちろんカレー臭はしなかった！ 品質もそこそこなインド製携帯電話\n",
              "364   it-life-hack      0  NaN  Twitterの影響力マジパねぇ！　東大駒場校舎で行われた無料耳かきサービスに人だかり\n",
              "661   it-life-hack      0  NaN              孫社長が新スマホを大々的に宣伝！放射線測定スマホを開発した理由\n",
              "27    it-life-hack      0  NaN           タダ者じゃない！7インチタブレット「eden TAB」の魅力をさぐる"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAeRakOUsast"
      },
      "source": [
        "# データの抽出\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZdJPKGZsj9s"
      },
      "source": [
        "# 1. BERT Tokenizerを用いて単語分割・IDへ変換\n",
        "## Tokenizerの準備\n",
        "from transformers import BertJapaneseTokenizer\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6io3Cs3uJv3",
        "outputId": "d5a08b8e-6353-48b1-ec28-c57d6666df2b"
      },
      "source": [
        "## テスト実行\n",
        "# 元文章\n",
        "print(' Original: ', sentences[0])\n",
        "# Tokenizer\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "# Token-id\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  旧式Macで禁断のパワーアップ！最新PCやソフトを一挙にチェック\n",
            "Tokenized:  ['旧式', 'Mac', 'で', '禁', '##断', 'の', 'パワーアップ', '!', '最新', 'PC', 'や', 'ソフト', 'を', '一挙', 'に', 'チェック']\n",
            "Token IDs:  [18718, 8653, 12, 1763, 29135, 5, 20734, 679, 6215, 3794, 49, 1604, 11, 24598, 7, 9398]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtdvYVhH481_",
        "outputId": "3f3c9751-7412-4c11-e7fc-d692b6a8cbfc"
      },
      "source": [
        "#最大単語数の確認\n",
        "max_len = []\n",
        "\n",
        "#1文づつ処理\n",
        "for sent in sentences:\n",
        "  #tokenizerで分割\n",
        "  token_words = tokenizer.tokenize(sent)\n",
        "  #文章数を取得してリストへ格納\n",
        "  max_len.append(len(token_words))\n",
        "#最大の数を確認\n",
        "print('最大単語数：', max(max_len))\n",
        "print('上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "最大単語数： 35\n",
            "上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPXImVQm6FN3",
        "outputId": "802209a3-21a2-44ab-a8f9-d023372242e4"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# 1文づつ処理\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, # Special Tokenの追加\n",
        "                        max_length = 37,           # 文章の長さを固定（Padding/Trancatinating）\n",
        "                        pad_to_max_length = True,# PADDINGで埋める\n",
        "                        return_attention_mask = True,   # Attention maksの作成\n",
        "                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n",
        "                   )\n",
        "\n",
        "    # 単語IDを取得    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # Attention　maskの取得\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# tenosor型に変換\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# 確認\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  旧式Macで禁断のパワーアップ！最新PCやソフトを一挙にチェック\n",
            "Token IDs: tensor([    2, 18718,  8653,    12,  1763, 29135,     5, 20734,   679,  6215,\n",
            "         3794,    49,  1604,    11, 24598,     7,  9398,     3,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqaUM6MnU1tF"
      },
      "source": [
        "ここまでは前処理を実施していたので、90%を訓練データ、10%をテストデータに分割して、pytorchで学習を行うためにデータローダーへ変換する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1m9O9xoUy4I",
        "outputId": "d602d57d-89ae-4523-c509-a50dff3cef3a"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#データセットクラスの作成 trainとtestをまとめる\n",
        "#https://kento1109.hatenablog.com/entry/2018/02/19/144109\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "#90%地点のIDを取得\n",
        "train_size = int(0.9*len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "#データセットを分割する\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('訓練データ数：{}'.format(train_size))\n",
        "print('検証データ数：{}'.format(val_size))\n",
        "\n",
        "#データローダーの作成\n",
        "batch_size = 32\n",
        "\n",
        "# 訓練データローダー\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), # ランダムにデータを取得してバッチ化\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "# 検証データローダー\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
        "            batch_size = batch_size\n",
        "        )\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "訓練データ数：1593\n",
            "検証データ数：177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfQTAcsJbQf9"
      },
      "source": [
        "学習済みモデルロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58V90cnjY3DR",
        "outputId": "74773f6f-80e4-4e67-84ba-a61b78805092"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# BertForSequenceClassification 学習済みモデルのロード\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"cl-tohoku/bert-base-japanese-whole-word-masking\", # 日本語Pre trainedモデルの指定\n",
        "    num_labels = 2, # ラベル数（今回はBinayなので2、数値を増やせばマルチラベルも対応可）\n",
        "    output_attentions = False, # アテンションベクトルを出力するか\n",
        "    output_hidden_states = False, # 隠れ層を出力するか\n",
        ")\n",
        "\n",
        "# モデルをGPUへ転送\n",
        "model.cuda()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQSRZ_libXF1"
      },
      "source": [
        "訓練(Pre-trainedモデルのファインチューニング)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAuBoO-EbLxv"
      },
      "source": [
        "# 最適化手法の設定\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 訓練パートの定義\n",
        "def train(model):\n",
        "    model.train() # 訓練モードで実行\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:# train_dataloaderはword_id, mask, labelを出力する点に注意\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs= model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss\n",
        "\n",
        "# テストパートの定義\n",
        "def validation(model):\n",
        "    model.eval()# 訓練モードをオフ\n",
        "    val_loss = 0\n",
        "    with torch.no_grad(): # 勾配を計算しない\n",
        "        for batch in validation_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            with torch.no_grad():        \n",
        "                (loss, logits) = model(b_input_ids, \n",
        "                                    token_type_ids=None, \n",
        "                                    attention_mask=b_input_mask,\n",
        "                                    labels=b_labels)\n",
        "            val_loss += loss.item()\n",
        "    return val_loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uArktKlkb5XP"
      },
      "source": [
        "学習に必要な関数が定義できたので、学習を行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vRRiao5b3am"
      },
      "source": [
        "# 学習の実行\n",
        "max_epoch = 4\n",
        "train_loss_ = []\n",
        "test_loss_ = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    train_ = train(model)\n",
        "    test_ = train(model)\n",
        "    train_loss_.append(train_)\n",
        "    test_loss_.append(test_)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RbjKx92c1zG"
      },
      "source": [
        "検証"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEwR09CVc2xW"
      },
      "source": [
        "# 検証方法の確認（1バッチ分で計算ロジックに確認）\n",
        "\n",
        "model.eval()# 訓練モードをオフ\n",
        "for batch in validation_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    with torch.no_grad():   \n",
        "        # 学習済みモデルによる予測結果をpredsで取得     \n",
        "        preds = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04wwA9igdZPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4b11ae-8cd9-4d6d-85be-d2ebc3dcb710"
      },
      "source": [
        "## 予測結果の確認\n",
        "print(f'出力:{preds}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "出力:SequenceClassifierOutput(loss=None, logits=tensor([[ 3.8569, -5.1612],\n",
            "        [ 4.1836, -5.6198],\n",
            "        [-3.7384,  5.3962],\n",
            "        [-4.3365,  5.6296],\n",
            "        [-4.3556,  5.5643],\n",
            "        [ 4.1479, -5.5610],\n",
            "        [-4.3778,  5.5701],\n",
            "        [-4.3649,  5.5774],\n",
            "        [ 4.1382, -5.6746],\n",
            "        [-4.3021,  5.5961],\n",
            "        [ 3.9430, -5.5044],\n",
            "        [ 4.0902, -5.6656],\n",
            "        [ 4.1682, -5.5363],\n",
            "        [ 4.1337, -5.5054],\n",
            "        [-4.3386,  5.6267],\n",
            "        [-4.2808,  5.5971],\n",
            "        [ 4.1567, -5.6577]], device='cuda:0'), hidden_states=None, attentions=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY1U2vhddZ2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "20de7d27-20c9-4e43-a159-7ae14d52338c"
      },
      "source": [
        "# 比較しやすい様にpd.dataframeへ整形\n",
        "import pandas as pd\n",
        "# pd.dataframeへ変換（GPUに乗っているTensorはgpu->cpu->numpy->dataframeと変換）\n",
        "logits_df = pd.DataFrame(preds[0].cpu().numpy(), columns=['logit_0', 'logit_1'])\n",
        "## np.argmaxで大き方の値を取得\n",
        "pred_df = pd.DataFrame(np.argmax(preds[0].cpu().numpy(), axis=1), columns=['pred_label'])\n",
        "label_df = pd.DataFrame(b_labels.cpu().numpy(), columns=['true_label'])\n",
        "\n",
        "accuracy_df = pd.concat([logits_df, pred_df, label_df], axis=1)\n",
        "\n",
        "accuracy_df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>logit_0</th>\n",
              "      <th>logit_1</th>\n",
              "      <th>pred_label</th>\n",
              "      <th>true_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.856918</td>\n",
              "      <td>-5.161248</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.183552</td>\n",
              "      <td>-5.619831</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3.738379</td>\n",
              "      <td>5.396243</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4.336545</td>\n",
              "      <td>5.629647</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.355567</td>\n",
              "      <td>5.564263</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    logit_0   logit_1  pred_label  true_label\n",
              "0  3.856918 -5.161248           0           0\n",
              "1  4.183552 -5.619831           0           0\n",
              "2 -3.738379  5.396243           1           1\n",
              "3 -4.336545  5.629647           1           1\n",
              "4 -4.355567  5.564263           1           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}